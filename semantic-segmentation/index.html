<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Semantic Segmentation</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Semantic Segmentation</h1>
	
        <p>Semantic segmentation is a field of computer vision, where
        its goal is to assign each pixel of a given image to one of
        the predefined class labels, e.g., road, pedestrian, vehicle,
        etc. By estimating the class of each pixel, one could
        understand the scene depicted on a given image. This page
        describes an application of a fully convolutional network
        (FCN) for semantic segmentation.</p>
	
        <p class="view"><a href="http://github.com/orderedlist/minimal">View
        the Project on  GitHub</a></p>
	
      </header>
      
      <section>
        <h1>Training a Fully Convolutional Network (FCN) for Semantic Segmentation</h1>

	<h2>1. Overview</h2>
        <p>Semantic segmentation is a computer vision task of
        assigning each pixel of a given image to one of the predefined
        class labels, e.g., road, pedestrian, vehicle, etc. By doing
        so, one can delineate contours of all the objects appearing on
        the input image.</p>

        <h2>2. Fully Convolutional Network (FCN) for Semantic Segmentation</h2>

	<p>This section describes how to train a fully convolutional
	network for a pixelwise, binary classification: road or
	  not-road.</p>

	<p>A convolutional neural network is typically comprised of a
	series of convolution layers, followed by fully connected
	layers and ultimately a soft max activation function. Such
	architectures work really well for classification -- remember
	how do all these frenzies about deep neural network begin?
	Yes, <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a>
	improved the classification accuracy drastically by dropping
	the error from 26% to 15%!</p>

	<img src="/semantic-segmentation/fig/yolo-output.png"/><a href="https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088">Source</a>
	<img src="/semantic-segmentation/fig/ssd-output.png"/><a href="https://arxiv.org/abs/1512.02325">Source</a>	
	<p>A typical output of a classification is a bounding box
	around an object of interest. See the above images. These
	bounding boxes will tell us/robots where objects in the scene
	are so that our robots could navigate the scene without
	bumping into them. There have been so many great
	classification
	algorithms: <a href="https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf">SVMs
	with
	HoG</a>, <a href="https://pjreddie.com/darknet/yolo/">Yolo</a>, <a href="https://medium.com/@jonathan_hui/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06">SSD</a>,
	etc.</p>

	<p>But what if we want to know the pixel positions of a road's
	boundary, instead of the bounding boxes of cars on that road?
	Can our awesome CNN (Convolutional Neural Network) do such a
	task for us? No, they cannot because the fully connected
	layers at the end of any CNNs do not preserve spatial
	information. The fully convolutional network (FCN) can do it
	for us. This is because, while doing the convolution, they
	preserve the spatial information throughout the entire
	network. In addition, since convolutional operations
	fundamentally don't care about the size of the input, a fully
	convolutional network will work on images of any size. In a
	classic convolutional network with fully connected final
	layers, the size of the input is constrained by the size of
	the fully connected layers. Passing different size images
	through the same sequence of convolutional layers and
	flattening the final output. These outputs would be of
	different sizes, which doesn't bode very well for matrix
	multiplication. Read the followings to learn more about
	DNN-based semantic segmentation.</p>

	<ul>
	  <li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review">A
	  2017 guide to semantic segmentation with deep
	      learning</a></li>
	  <li><a href="http://deeplearning.net/tutorial/fcn_2D_segm.html">Fully convolution networks for 2d segmentation</a></li>
	</ul>

	<p>Well, then how does a FCN achieve such a task? It can do
	such a task for us based, primarily, on three special
	  techniques:
	  <ul>
	    <li>1x1 convolutioinal layers,</li>
	    <li>up-sampling, and</li>
	    <li>skip connections.</li>
	  </ul></p>

	<p>In general, a FCN is typically comprised of two parts:
	  encoder and decoder. A encoder extracts features that will
	  later be used by the decoder. Since this is conceptually
	  same as a transfer learning, we can use techniques from
	  transfer learning to accelerate the training of our
	  FCNs. It's common to use the encoder pre-trained on
	  ImageNet. As examples, VGG and ResNet are popular
	  choices. By applying the first special technique of 1x1
	  convolutional layer conversion, we can complete the encoder
	  portion of the FCN. The encoder is followed by the decoder,
	  which uses a second special technique of transposed
	  convolutional layers to upsample the image. Then the skip
	  connection via the third special technique is added. While
	  utilizing the skip connections, one'll have to be careful
	  not to add too many skip connections. Otherwise it can lead
	  to the explosion in the size of your model. For example,
	  when using VGG-16 as the encoder, only the third and the
	  fourth pooling layers are typically used for skip
	  connections. Once we're done with incorporating all these
	  three techniques, we can train the model end to end as we do
	  for a CNN. Read the followings to learn more about encoder
	  and decoder architecture.
	  <ul>
	    <li><a href="https://arxiv.org/pdf/1511.00561.pdf">SegNet:
	A deep convolutional encoder-decoder architecture for image
		segmentation</a></li>
	    <li><a href="https://courses.cs.washington.edu/courses/cse576/17sp/notes/Sachin_Talk.pdf">Encoder-decoder networks for semantic segmentation</a></li>
	  </ul>

	  Now let's take a look at each of the three techniques in
	  details:
	</p>

	<h3>2.1 1x1 Convolution Layer</h3>

	<p>What do we mean by using 1x1 convolutional layers? It is to
	replace a fully connected layer with 1x1 convolutional
	layers. By doing so, the output of the convolution operation
	is the result of sweeping the kernel over the input with the
	sliding window and performing element wise multiplication and
	summation. One way to think about this is the number of
	kernels is equivalent to the number of outputs in a fully
	connected layer. Similarly, the number of weights in each
	kernel is equivalent to the number of inputs in the fully
	connected layer. Effectively, this turns convolutions into a
	  matrix multiplication with spatial information.</p>

	<p>In TensorFlow, one can implement 1x1 convolutional layer as: </p>
<pre class="brush: python">
  tf.layers.conv2d(x, num_outputs, kernel_size=1, stride=1, weights_initializer=custom_init)
</pre>
	<p>where,
	  <ul>
	    <li>num_outputs: the number of output channels or kernels</li>
	    <li><a href="http://cs231n.github.io/neural-networks-2/#init">weight initialization</a></li>
	    <li><a href="http://cs231n.github.io/neural-networks-2/#reg">regularization</a></li>
	  </ul>
	</p>
	  
	<h3>2.2 Up-sampling </h3>

	<p>An up-sampling is done through a transposed convolution. A
	transposed convolution is essentially a reverse convolution
	where the forward and the backward passes are swapped -- this
	is why it is called transpose convolution. Some people call it
	deconvolution because it literally undoes the previous
	convolution. Note that the math is exactly the same because
	what we're doing is swapping the order of forward and backward
	  passes.</p>

	<p>Such transposed convolutions help in upsampling the
	  previous layer to a higher resolution or dimension. Let's
	  take an example: Suppose you have a 3x3 input and you wish
	  to upsample that to the desired dimension of 6x6. The
	  process involves multiplying each pixel of your input with a
	  kernel or filter. If this filter was of size 5x5, the output
	  of this operation will be a weighted kernel of size
	  5x5. This weighted kernel then defines your output
	  layer. However, the upsampling part of the process is
	  defined by the strides and the padding. In TensorFlow, using
	  the `tf.layers.conv2d_transpose`, a `stride` of 2, and
	  `"SAME"` padding would result in an output of dimensions
	  6x6. See the followings to learn more about up-sampling.</p>
	<ul>
	  <li><a href="https://github.com/vdumoulin/conv_arithmetic">Animations about convolution arithmetic</a></li>	  
	  <li><a href="https://arxiv.org/abs/1603.07285">A guide to convolution arithmetic for deep learning</a></li>
	</ul>

	<p>In TensorFlow, </p>
<pre class="brush: python">
  tf.layers.conv2d_transpose(x, 3, (2, 2), (2,2))
</pre>

	<h3>2.3 Skip Connections</h3>
	<p>The third special technique that a FCN uses is the skip
	connection. One effect of convolutions or encoding in general
	is you narrow down the scope by looking closely at some
	picture and lose the bigger picture as a result. So even if we
	were to decode the output of the encoder back to the original
	image size, some information has been lost. Skip connections
	are a way of retaining the information easily. The way skip
	connection work is by connecting the output of one layer to a
	non-adjacent layer. </p>

	<h3>2.4 Implementation of a FCN</h3>
	<p>This section describes how to train a fully convolutional
	network for a pixelwise, binary classification: road or
	  not-road.</p>
	  
	<p>The FCN we are primarily based on is
	the <a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf">FCN-8</a>

	<pre class="brush: python">
def load_vgg(sess, vgg_path):
    """
    Load a pre-trained network, VGG for this case, into TensorFlow as an encoder.

    :param sess: TensorFlow Session
    :param vgg_path: Path to vgg folder, containing "variables/" and "saved_model.pb"

    :return: Tensors from VGG model (image_input, keep_prob, layer3_out, layer4_out, layer7_out)
    """

    vgg_tag = 'vgg16'
    vgg_input_tensor_name = 'image_input:0'
    vgg_keep_prob_tensor_name = 'keep_prob:0'
    vgg_layer3_out_tensor_name = 'layer3_out:0'
    vgg_layer4_out_tensor_name = 'layer4_out:0'
    vgg_layer7_out_tensor_name = 'layer7_out:0'

    # Use tf.saved_model.loader.load to load the model and weights
    # tf.saved_model.loader.load() returns a protobuf
    tf.saved_model.loader.load(sess, [vgg_tag], vgg_path)

    # Get tensors to return
    image_input = sess.graph.get_tensor_by_name(vgg_input_tensor_name)
    keep_prob = sess.graph.get_tensor_by_name(vgg_keep_prob_tensor_name)
    layer3_out = sess.graph.get_tensor_by_name(vgg_layer3_out_tensor_name)
    layer4_out = sess.graph.get_tensor_by_name(vgg_layer4_out_tensor_name)
    layer7_out = sess.graph.get_tensor_by_name(vgg_layer7_out_tensor_name)

    return image_input, keep_prob, layer3_out, layer4_out, layer7_out's
	</pre>

	<h2>3. Results</h2>

	<h3>3.1 Setup</h3>
	In order to make the above described code run, you'll need GPU. The packages you need
	<ul>
	  <li><a href="https://www.python.org">Python 3</a></li>
	  <li><a href="https://www.tensorflow.org">TensorFlow</a></li>
	  <li><a href="https://www.numpy.org">Numpy</a></li>
	  <li><a href="https://www.scipy.org">SciPy</a></li>
	</ul>

	<h3>3.2 Dataset</h3>
	There is a couple of dataset for semantic segmentation.
	<ul>
	  <li><a href="http://www.cvlibs.net/datasets/kitti/eval_road.php">KITTI
	      Road Dataset</a></li>
	  <li><a href="https://www.cityscapes-dataset.com/examples/">CityScapes Dataset</a></li>
	  <li><a href="http://bdd-data.berkeley.edu/">Berkeley DeepDrive Dataset</a></li>
	</ul>
	    

	<h3>3.3 Results</h3>
	<p>The animated image at the below shows some of the
	  segmentation results.</p>

	<img src="/semantic-segmentation/fig/semantic-segmentation-output.gif"/>
	
	<h2>. Summary</h2>
	
        <h2>Set in stone</h2>

        <p>Preformatted blocks are useful for ASCII art:</p>

        <pre>             ,-.
    ,     ,-.   ,-.
   / \   (   )-(   )
   \ |  ,.&gt;-(   )-&lt;
    \|,' (   )-(   )
     Y ___`-'   `-'
     |/__/   `-'
     |
     |
     |    -hrr-
  ___|_____________
</pre>

        <h2>Playing the blame game</h2>

        <p>If you need to blame someone, the best way to do so is by quoting them:</p>

        <blockquote>
        <p>I, at any rate, am convinced that He does not throw dice.</p>
        </blockquote>

        <p>Or perhaps someone a little less eloquent:</p>

        <blockquote>
        <p>I wish you'd have given me this written question ahead of time so I<br>
        could plan for it... I'm sure something will pop into my head here in<br>
        the midst of this press conference, with all the pressure of trying to<br>
        come up with answer, but it hadn't yet...</p>

        <p>I don't want to sound like<br>
        I have made no mistakes. I'm confident I have. I just haven't - you<br>
        just put me under the spot here, and maybe I'm not as quick on my feet<br>
        as I should be in coming up with one.</p>
        </blockquote>

        <h2>Table for two</h2>

        <table>
          <tbody>
        <tr>
            <th>ID</th>
        <th>Name</th>
        <th>Rank</th>
          </tr>
          <tr>
            <td>1</td>
        <td>Tom Preston-Werner</td>
        <td>Awesome</td>
          </tr>
          <tr>
            <td>2</td>
        <td>Albert Einstein</td>
        <td>Nearly as awesome</td>
          </tr>
        </tbody>
        </table>
      </section>
      <footer>
        <p>This page is scribbled by <a href="http://ywseo.github.io/">Youngwoo Seo</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
  </body>
</html>
